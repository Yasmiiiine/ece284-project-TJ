{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce505521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16_Part1 模型构建完成。目标 8x8 层已插入且去除了 BN。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from models import * # 假设你的 QuantConv2d 等定义在 models.py 中\n",
    "\n",
    "class VGG16_Part1(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16_Part1, self).__init__()\n",
    "        \n",
    "        # 我们手动构建 features，以便精准插入修改层\n",
    "        # 注意：这里假设你使用 CIFAR-10 类似的输入尺寸 (32x32)\n",
    "        self.features = nn.Sequential(\n",
    "            # --- Block 1 ---\n",
    "            QuantConv2d(3, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            QuantConv2d(64, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # --- Block 2 ---\n",
    "            QuantConv2d(64, 128, kernel_size=3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            QuantConv2d(128, 128, kernel_size=3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # --- Block 3 ---\n",
    "            QuantConv2d(128, 256, kernel_size=3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            QuantConv2d(256, 256, kernel_size=3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            QuantConv2d(256, 256, kernel_size=3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # --- Block 4 ---\n",
    "            QuantConv2d(256, 512, kernel_size=3, padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
    "            QuantConv2d(512, 512, kernel_size=3, padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
    "            QuantConv2d(512, 512, kernel_size=3, padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # --- Block 5 (修改区域) ---\n",
    "            # 原本的第一层: 512 -> 512\n",
    "            QuantConv2d(512, 512, kernel_size=3, padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
    "            \n",
    "            # ================= PART 1 核心修改: 8x8 Squeezed Layer =================\n",
    "            # 1. 适配层 (Adapter): 512 -> 8 (使用 1x1 卷积降维)\n",
    "            QuantConv2d(512, 8, kernel_size=1), \n",
    "            nn.BatchNorm2d(8), # 这个BN可以保留，帮助收敛\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 2. 目标层 (Target Layer for FPGA): 8 -> 8 (3x3 卷积)\n",
    "            # 要求: 8 input, 8 output, NO Batch Norm\n",
    "            QuantConv2d(8, 8, kernel_size=3, padding=1, bias=False), \n",
    "            nn.ReLU(inplace=True), # 直接接 ReLU，没有 BN\n",
    "            \n",
    "            # 3. 恢复层 (Expand): 8 -> 512 (使用 1x1 卷积升维，接回原网络)\n",
    "            QuantConv2d(8, 512, kernel_size=1), \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # ===================================================================\n",
    "            \n",
    "            # Block 5 剩余部分\n",
    "            QuantConv2d(512, 512, kernel_size=3, padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.AvgPool2d(kernel_size=1, stride=1),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model_part1 = VGG16_Part1().cuda()\n",
    "print(\"VGG16_Part1 模型构建完成。目标 8x8 层已插入且去除了 BN。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b47b550f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'result/vgg16/model_best.pth.tar'\n",
      "已加载预训练权重。忽略了 63 个不匹配的参数层（这些层将从头训练）。\n"
     ]
    }
   ],
   "source": [
    "def load_pretrained_weights(model, pretrained_path):\n",
    "    if os.path.isfile(pretrained_path):\n",
    "        print(f\"=> loading checkpoint '{pretrained_path}'\")\n",
    "        checkpoint = torch.load(pretrained_path)\n",
    "        pretrained_dict = checkpoint['state_dict']\n",
    "        model_dict = model.state_dict()\n",
    "        \n",
    "        # 1. 过滤掉形状不匹配的层 (即我们修改过的 Block 5 部分)\n",
    "        # 2. 过滤掉名字不匹配的层 (虽然大部分名字应该匹配)\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() \n",
    "                           if k in model_dict and v.shape == model_dict[k].shape}\n",
    "        \n",
    "        # 更新当前模型权重\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        \n",
    "        print(f\"已加载预训练权重。忽略了 {len(model.state_dict()) - len(pretrained_dict)} 个不匹配的参数层（这些层将从头训练）。\")\n",
    "    else:\n",
    "        print(f\"在 '{pretrained_path}' 未找到 checkpoint\")\n",
    "\n",
    "# 请将此路径替换为你实际存放 >90% 模型的路径\n",
    "PRETRAINED_PATH = \"result/vgg16/model_best.pth.tar\" \n",
    "load_pretrained_weights(model_part1, PRETRAINED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e868e95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据加载器 (trainloader, testloader) 已准备就绪！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 1. 定义预处理 (Normalization)\n",
    "# 这些均值和方差是 CIFAR-10 的标准参数\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "# 2. 训练集加载器 (Train Loader)\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4), # 数据增强\n",
    "        transforms.RandomHorizontalFlip(),    # 数据增强\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=True, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# 3. 测试集加载器 (Test Loader)\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=False, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(\"数据加载器 (trainloader, testloader) 已准备就绪！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a175d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "启动增强版训练 (含学习率衰减)...\n",
      "Epoch [1/50] (LR: 0.01000) -> Test Accuracy: 83.58%\n",
      "   New Best found: 83.58% (Saved)\n",
      "Epoch [2/50] (LR: 0.01000) -> Test Accuracy: 81.42%\n",
      "Epoch [3/50] (LR: 0.01000) -> Test Accuracy: 82.62%\n",
      "Epoch [4/50] (LR: 0.01000) -> Test Accuracy: 83.05%\n",
      "Epoch [5/50] (LR: 0.01000) -> Test Accuracy: 82.51%\n",
      "Epoch [6/50] (LR: 0.01000) -> Test Accuracy: 85.77%\n",
      "   New Best found: 85.77% (Saved)\n",
      "Epoch [7/50] (LR: 0.01000) -> Test Accuracy: 86.35%\n",
      "   New Best found: 86.35% (Saved)\n",
      "Epoch [8/50] (LR: 0.01000) -> Test Accuracy: 86.62%\n",
      "   New Best found: 86.62% (Saved)\n",
      "Epoch [9/50] (LR: 0.01000) -> Test Accuracy: 84.95%\n",
      "Epoch [10/50] (LR: 0.01000) -> Test Accuracy: 86.94%\n",
      "   New Best found: 86.94% (Saved)\n",
      "Epoch [11/50] (LR: 0.01000) -> Test Accuracy: 86.81%\n",
      "Epoch [12/50] (LR: 0.01000) -> Test Accuracy: 87.04%\n",
      "   New Best found: 87.04% (Saved)\n",
      "Epoch [13/50] (LR: 0.01000) -> Test Accuracy: 88.09%\n",
      "   New Best found: 88.09% (Saved)\n",
      "Epoch [14/50] (LR: 0.01000) -> Test Accuracy: 88.34%\n",
      "   New Best found: 88.34% (Saved)\n",
      "Epoch [15/50] (LR: 0.01000) -> Test Accuracy: 85.67%\n",
      "Epoch [16/50] (LR: 0.01000) -> Test Accuracy: 87.46%\n",
      "Epoch [17/50] (LR: 0.01000) -> Test Accuracy: 87.02%\n",
      "Epoch [18/50] (LR: 0.01000) -> Test Accuracy: 87.55%\n",
      "Epoch [19/50] (LR: 0.01000) -> Test Accuracy: 86.53%\n",
      "Epoch [20/50] (LR: 0.00100) -> Test Accuracy: 87.51%\n",
      "Epoch [21/50] (LR: 0.00100) -> Test Accuracy: 90.91%\n",
      "   New Best found: 90.91% (Saved)\n",
      "Epoch [22/50] (LR: 0.00100) -> Test Accuracy: 91.13%\n",
      "   New Best found: 91.13% (Saved)\n",
      "Epoch [23/50] (LR: 0.00100) -> Test Accuracy: 91.19%\n",
      "   New Best found: 91.19% (Saved)\n",
      "Epoch [24/50] (LR: 0.00100) -> Test Accuracy: 91.38%\n",
      "   New Best found: 91.38% (Saved)\n",
      "Epoch [25/50] (LR: 0.00100) -> Test Accuracy: 91.56%\n",
      "   New Best found: 91.56% (Saved)\n",
      "Epoch [26/50] (LR: 0.00100) -> Test Accuracy: 91.35%\n",
      "Epoch [27/50] (LR: 0.00100) -> Test Accuracy: 91.42%\n",
      "Epoch [28/50] (LR: 0.00100) -> Test Accuracy: 91.27%\n",
      "Epoch [29/50] (LR: 0.00100) -> Test Accuracy: 91.30%\n",
      "Epoch [30/50] (LR: 0.00100) -> Test Accuracy: 91.33%\n",
      "Epoch [31/50] (LR: 0.00100) -> Test Accuracy: 90.98%\n",
      "Epoch [32/50] (LR: 0.00100) -> Test Accuracy: 91.43%\n",
      "Epoch [33/50] (LR: 0.00100) -> Test Accuracy: 91.46%\n",
      "Epoch [34/50] (LR: 0.00100) -> Test Accuracy: 91.48%\n",
      "Epoch [35/50] (LR: 0.00100) -> Test Accuracy: 91.63%\n",
      "   New Best found: 91.63% (Saved)\n",
      "Epoch [36/50] (LR: 0.00100) -> Test Accuracy: 91.34%\n",
      "Epoch [37/50] (LR: 0.00100) -> Test Accuracy: 91.89%\n",
      "   New Best found: 91.89% (Saved)\n",
      "Epoch [38/50] (LR: 0.00100) -> Test Accuracy: 91.35%\n",
      "Epoch [39/50] (LR: 0.00100) -> Test Accuracy: 91.21%\n",
      "Epoch [40/50] (LR: 0.00010) -> Test Accuracy: 91.48%\n",
      "Epoch [41/50] (LR: 0.00010) -> Test Accuracy: 91.63%\n",
      "Epoch [42/50] (LR: 0.00010) -> Test Accuracy: 91.53%\n",
      "Epoch [43/50] (LR: 0.00010) -> Test Accuracy: 91.49%\n",
      "Epoch [44/50] (LR: 0.00010) -> Test Accuracy: 91.50%\n",
      "Epoch [45/50] (LR: 0.00010) -> Test Accuracy: 91.54%\n",
      "Epoch [46/50] (LR: 0.00010) -> Test Accuracy: 91.79%\n",
      "Epoch [47/50] (LR: 0.00010) -> Test Accuracy: 91.59%\n",
      "Epoch [48/50] (LR: 0.00010) -> Test Accuracy: 91.79%\n",
      "Epoch [49/50] (LR: 0.00010) -> Test Accuracy: 91.76%\n",
      "Epoch [50/50] (LR: 0.00010) -> Test Accuracy: 91.62%\n",
      " Advanced Fine-tuning 完成！最终最佳精度: 91.89%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91.89"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 1. 重新定义优化器 (初始 LR = 0.01)\n",
    "optimizer = optim.SGD(model_part1.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# 2. 关键改进：添加学习率调度器 (LR Scheduler)\n",
    "# 在第 20 轮和第 40 轮时，将学习率乘以 0.1 (即变为 0.001 和 0.0001)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 40], gamma=0.1)\n",
    "\n",
    "# 3. 定义 Loss\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "def fine_tune_advanced(model, train_loader, test_loader, epochs=50):\n",
    "    best_acc = 0\n",
    "    model.cuda()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        for i, (input, target) in enumerate(train_loader):\n",
    "            input, target = input.cuda(), target.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "            \n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for input, target in test_loader:\n",
    "                input, target = input.cuda(), target.cuda()\n",
    "                output = model(input)\n",
    "                _, predicted = output.max(1)\n",
    "                total += target.size(0)\n",
    "                correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        acc = 100. * correct / total\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] (LR: {current_lr:.5f}) -> Test Accuracy: {acc:.2f}%\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save({'state_dict': model.state_dict()}, \"result/vgg_16_part1_best.pth.tar\")\n",
    "            print(f\"   New Best found: {best_acc:.2f}% (Saved)\")\n",
    "            \n",
    "    print(f\" Advanced Fine-tuning 完成！最终最佳精度: {best_acc:.2f}%\")\n",
    "    return best_acc\n",
    "\n",
    "# 4. 开始训练 (这次跑 50 轮，请耐心等待)\n",
    "print(\"启动增强版训练 (含学习率衰减)...\")\n",
    "fine_tune_advanced(model_part1, trainloader, testloader, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1c5c3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在准备生成验证文件...\n",
      "正在模型中搜索 8x8 卷积层...\n",
      "找到目标层: features[40] (In:8 -> Out:8)\n",
      " 正在运行一次前向传播以捕获数据...\n",
      "捕获输入数据形状: torch.Size([8, 2, 2])\n",
      "权重形状: (8, 8, 3, 3)\n",
      "正在计算 Golden Output (Psum Recovered)...\n",
      "========================================\n",
      "   成功！验证文件已保存在: part1_verification_data/\n",
      "   - part1_verification_data/input.txt  (大小: 32)\n",
      "   - part1_verification_data/weight.txt (大小: 576)\n",
      "   - part1_verification_data/output.txt (大小: 32)\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def generate_part1_verification_files(model, test_loader):\n",
    "    print(\"正在准备生成验证文件...\")\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. 定义钩子 (Hook) 用于捕获中间层数据\n",
    "    captured_data = {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            # input[0] 是进入该层的激活值\n",
    "            captured_data[name] = input[0].detach()\n",
    "        return hook\n",
    "\n",
    "    # 2. 自动寻找目标 8x8 层\n",
    "    target_layer = None\n",
    "    target_layer_name = \"\"\n",
    "    \n",
    "    print(\"正在模型中搜索 8x8 卷积层...\")\n",
    "    for name, module in model.features.named_children():\n",
    "        # 检查是否为 QuantConv2d 且输入输出通道均为 8\n",
    "        if hasattr(module, 'in_channels') and module.in_channels == 8 and module.out_channels == 8:\n",
    "            target_layer = module\n",
    "            target_layer_name = name\n",
    "            print(f\"找到目标层: features[{name}] (In:8 -> Out:8)\")\n",
    "            break\n",
    "    \n",
    "    if target_layer is None:\n",
    "        print(\"错误：未在模型中找到 8x8 的目标层。请检查 VGG16_Part1 类的定义是否正确。\")\n",
    "        return\n",
    "\n",
    "    # 3. 注册 Hook\n",
    "    handle = target_layer.register_forward_hook(get_activation(\"target_layer\"))\n",
    "    \n",
    "    # 4. 运行一次前向传播 (取一个 Batch)\n",
    "    print(\" 正在运行一次前向传播以捕获数据...\")\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    images = images.cuda()\n",
    "    with torch.no_grad():\n",
    "        model(images)\n",
    "    \n",
    "    # 5. 提取数据并转换为整数 (模拟硬件行为)\n",
    "    if \"target_layer\" not in captured_data:\n",
    "        print(\"错误：Hook 未捕获到数据。\")\n",
    "        return\n",
    "\n",
    "    # 获取激活输入 (Batch, 8, H, W) -> 取第一张图 [0]\n",
    "    act_float = captured_data[\"target_layer\"][0] # Shape: [8, H, W]\n",
    "    print(f\"捕获输入数据形状: {act_float.shape}\")\n",
    "    \n",
    "    # 获取量化参数\n",
    "    # 兼容性处理：检查 alpha 是 Tensor 还是 float\n",
    "    try:\n",
    "        act_alpha = target_layer.act_alpha.item() if torch.is_tensor(target_layer.act_alpha) else target_layer.act_alpha\n",
    "        wgt_alpha = target_layer.weight_quant.wgt_alpha.item() if torch.is_tensor(target_layer.weight_quant.wgt_alpha) else target_layer.weight_quant.wgt_alpha\n",
    "    except AttributeError:\n",
    "        print(\"警告：无法读取标准量化参数，尝试使用默认值或检查模型结构。\")\n",
    "        act_alpha = 10.0 # 默认值，仅作防止崩溃用\n",
    "        wgt_alpha = 1.0\n",
    "    \n",
    "    w_bit = 4\n",
    "    a_bit = 4\n",
    "    \n",
    "    # --- 转换输入 (Input) ---\n",
    "    # Quantize: round(x / step)\n",
    "    act_step = act_alpha / (2**a_bit - 1)\n",
    "    # 模拟硬件：输入通常是无符号的 4-bit (0~15) 或者有符号 (根据你的硬件设计，通常 ReLU 后是无符号)\n",
    "    # 这里假设 ReLU 后输入为正，映射到 0-15\n",
    "    input_int = torch.round(act_float / act_step).clamp(0, 15).cpu().numpy().astype(int)\n",
    "    \n",
    "    # --- 转换权重 (Weight) ---\n",
    "    # 权重通常是有符号 4-bit (-8 ~ 7)\n",
    "    wgt_step = wgt_alpha / (2**(w_bit - 1) - 1)\n",
    "    weight_int = torch.round(target_layer.weight / wgt_step).clamp(-8, 7).detach().cpu().numpy().astype(int)\n",
    "    \n",
    "    print(f\"权重形状: {weight_int.shape}\")\n",
    "    \n",
    "    # --- 计算预期输出 (Golden Output) ---\n",
    "    print(\"正在计算 Golden Output (Psum Recovered)...\")\n",
    "    # 使用 PyTorch 的 Conv2d 来模拟整数卷积\n",
    "    # 硬件公式: Psum = Input_Int * Weight_Int (累加)\n",
    "    sim_conv = nn.Conv2d(8, 8, kernel_size=3, padding=1, bias=False)\n",
    "    sim_conv.weight = nn.Parameter(torch.tensor(weight_int).float())\n",
    "    \n",
    "    input_tensor = torch.tensor(input_int).unsqueeze(0).float() # Add batch dim\n",
    "    \n",
    "    # 计算整数 Psum\n",
    "    output_int_psum = sim_conv(input_tensor)\n",
    "    \n",
    "    # 模拟 ReLU (硬件最后一步)\n",
    "    output_int_final = torch.relu(output_int_psum).detach().numpy().astype(int)\n",
    "    \n",
    "    # 6. 保存为 TXT 文件 (供 RTL 读取)\n",
    "    output_dir = \"part1_verification_data\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # 保存 Weight (Flattened) - 硬件读取顺序\n",
    "    np.savetxt(f\"{output_dir}/weight.txt\", weight_int.flatten(), fmt='%d')\n",
    "    \n",
    "    # 保存 Input (Flattened)\n",
    "    np.savetxt(f\"{output_dir}/input.txt\", input_int.flatten(), fmt='%d')\n",
    "    \n",
    "    # 保存 Output (Flattened)\n",
    "    np.savetxt(f\"{output_dir}/output.txt\", output_int_final.flatten(), fmt='%d')\n",
    "    \n",
    "    print(\"=\"*40)\n",
    "    print(f\"   成功！验证文件已保存在: {output_dir}/\")\n",
    "    print(f\"   - {output_dir}/input.txt  (大小: {input_int.size})\")\n",
    "    print(f\"   - {output_dir}/weight.txt (大小: {weight_int.size})\")\n",
    "    print(f\"   - {output_dir}/output.txt (大小: {output_int_final.size})\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    handle.remove()\n",
    "\n",
    "#  激活调用：生成文件\n",
    "generate_part1_verification_files(model_part1, testloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
