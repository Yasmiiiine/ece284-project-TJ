{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "VGG_quant(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (25): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): QuantConv2d(\n",
      "      16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): QuantConv2d(\n",
      "      8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (30): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (33): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (41): ReLU(inplace=True)\n",
      "    (42): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (43): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "    \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *   # bring everything in the folder models\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_quant_2bit1\"\n",
    "model = VGG16_quant()\n",
    "print(model)\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [80, 120]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "junior-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 0.615 (0.615)\tData 0.114 (0.114)\tLoss 2.4758 (2.4758)\tPrec 10.156% (10.156%)\n",
      "Epoch: [0][100/391]\tTime 0.044 (0.051)\tData 0.001 (0.003)\tLoss 2.4063 (3.2931)\tPrec 7.812% (10.435%)\n",
      "Epoch: [0][200/391]\tTime 0.047 (0.048)\tData 0.001 (0.002)\tLoss 2.4781 (2.8279)\tPrec 12.500% (10.801%)\n",
      "Epoch: [0][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.002)\tLoss 2.3102 (2.6678)\tPrec 10.938% (11.078%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.116 (0.116)\tLoss 2.2623 (2.2623)\tPrec 14.844% (14.844%)\n",
      " * Prec 11.370% \n",
      "best acc: 11.370000\n",
      "Epoch: [1][0/391]\tTime 0.157 (0.157)\tData 0.122 (0.122)\tLoss 2.3877 (2.3877)\tPrec 10.938% (10.938%)\n",
      "Epoch: [1][100/391]\tTime 0.043 (0.048)\tData 0.001 (0.005)\tLoss 2.3085 (2.3142)\tPrec 7.812% (11.997%)\n",
      "Epoch: [1][200/391]\tTime 0.044 (0.046)\tData 0.001 (0.003)\tLoss 2.3162 (2.3094)\tPrec 15.625% (12.150%)\n",
      "Epoch: [1][300/391]\tTime 0.045 (0.046)\tData 0.006 (0.003)\tLoss 2.2923 (2.3038)\tPrec 11.719% (12.448%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.116 (0.116)\tLoss 2.2731 (2.2731)\tPrec 10.156% (10.156%)\n",
      " * Prec 13.080% \n",
      "best acc: 13.080000\n",
      "Epoch: [2][0/391]\tTime 0.162 (0.162)\tData 0.131 (0.131)\tLoss 2.3009 (2.3009)\tPrec 8.594% (8.594%)\n",
      "Epoch: [2][100/391]\tTime 0.054 (0.049)\tData 0.001 (0.005)\tLoss 2.2975 (2.2845)\tPrec 10.938% (13.312%)\n",
      "Epoch: [2][200/391]\tTime 0.036 (0.047)\tData 0.002 (0.003)\tLoss 2.3228 (2.2840)\tPrec 13.281% (13.468%)\n",
      "Epoch: [2][300/391]\tTime 0.041 (0.047)\tData 0.002 (0.003)\tLoss 2.2495 (2.2817)\tPrec 17.969% (13.455%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.117 (0.117)\tLoss 2.2765 (2.2765)\tPrec 12.500% (12.500%)\n",
      " * Prec 13.450% \n",
      "best acc: 13.450000\n",
      "Epoch: [3][0/391]\tTime 0.156 (0.156)\tData 0.123 (0.123)\tLoss 2.2445 (2.2445)\tPrec 14.844% (14.844%)\n",
      "Epoch: [3][100/391]\tTime 0.050 (0.048)\tData 0.001 (0.005)\tLoss 2.2313 (2.2705)\tPrec 14.844% (14.086%)\n",
      "Epoch: [3][200/391]\tTime 0.041 (0.047)\tData 0.002 (0.003)\tLoss 2.2626 (2.2654)\tPrec 18.750% (14.572%)\n",
      "Epoch: [3][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.003)\tLoss 2.2853 (2.2628)\tPrec 13.281% (14.735%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.126 (0.126)\tLoss 2.2469 (2.2469)\tPrec 14.844% (14.844%)\n",
      " * Prec 14.800% \n",
      "best acc: 14.800000\n",
      "Epoch: [4][0/391]\tTime 0.184 (0.184)\tData 0.151 (0.151)\tLoss 2.2710 (2.2710)\tPrec 11.719% (11.719%)\n",
      "Epoch: [4][100/391]\tTime 0.032 (0.048)\tData 0.002 (0.005)\tLoss 2.2436 (2.2536)\tPrec 18.750% (15.292%)\n",
      "Epoch: [4][200/391]\tTime 0.039 (0.047)\tData 0.002 (0.003)\tLoss 2.2130 (2.2522)\tPrec 16.406% (15.423%)\n",
      "Epoch: [4][300/391]\tTime 0.044 (0.047)\tData 0.001 (0.003)\tLoss 2.2550 (2.2493)\tPrec 17.969% (15.796%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.121 (0.121)\tLoss 2.2687 (2.2687)\tPrec 15.625% (15.625%)\n",
      " * Prec 14.680% \n",
      "best acc: 14.800000\n",
      "Epoch: [5][0/391]\tTime 0.176 (0.176)\tData 0.133 (0.133)\tLoss 2.2845 (2.2845)\tPrec 14.062% (14.062%)\n",
      "Epoch: [5][100/391]\tTime 0.049 (0.051)\tData 0.001 (0.006)\tLoss 2.2769 (2.2411)\tPrec 14.062% (16.112%)\n",
      "Epoch: [5][200/391]\tTime 0.045 (0.048)\tData 0.001 (0.004)\tLoss 2.2765 (2.2403)\tPrec 21.094% (16.224%)\n",
      "Epoch: [5][300/391]\tTime 0.051 (0.047)\tData 0.001 (0.003)\tLoss 2.2343 (2.2370)\tPrec 12.500% (16.422%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.114 (0.114)\tLoss 2.2444 (2.2444)\tPrec 16.406% (16.406%)\n",
      " * Prec 16.040% \n",
      "best acc: 16.040000\n",
      "Epoch: [6][0/391]\tTime 0.200 (0.200)\tData 0.165 (0.165)\tLoss 2.2356 (2.2356)\tPrec 17.188% (17.188%)\n",
      "Epoch: [6][100/391]\tTime 0.048 (0.047)\tData 0.002 (0.003)\tLoss 2.2568 (2.2240)\tPrec 18.750% (17.334%)\n",
      "Epoch: [6][200/391]\tTime 0.045 (0.046)\tData 0.002 (0.002)\tLoss 2.2600 (2.2222)\tPrec 12.500% (17.071%)\n",
      "Epoch: [6][300/391]\tTime 0.042 (0.046)\tData 0.002 (0.002)\tLoss 2.1736 (2.2191)\tPrec 17.969% (17.188%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.121 (0.121)\tLoss 2.1887 (2.1887)\tPrec 18.750% (18.750%)\n",
      " * Prec 17.120% \n",
      "best acc: 17.120000\n",
      "Epoch: [7][0/391]\tTime 0.169 (0.169)\tData 0.134 (0.134)\tLoss 2.2266 (2.2266)\tPrec 13.281% (13.281%)\n",
      "Epoch: [7][100/391]\tTime 0.044 (0.046)\tData 0.001 (0.003)\tLoss 2.0413 (2.1736)\tPrec 19.531% (18.340%)\n",
      "Epoch: [7][200/391]\tTime 0.040 (0.045)\tData 0.001 (0.002)\tLoss 2.2026 (2.1412)\tPrec 15.625% (19.092%)\n",
      "Epoch: [7][300/391]\tTime 0.043 (0.045)\tData 0.002 (0.002)\tLoss 2.0214 (2.1221)\tPrec 21.094% (19.599%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.117 (0.117)\tLoss 2.0946 (2.0946)\tPrec 17.969% (17.969%)\n",
      " * Prec 21.290% \n",
      "best acc: 21.290000\n",
      "Epoch: [8][0/391]\tTime 0.151 (0.151)\tData 0.119 (0.119)\tLoss 2.0637 (2.0637)\tPrec 19.531% (19.531%)\n",
      "Epoch: [8][100/391]\tTime 0.047 (0.047)\tData 0.001 (0.003)\tLoss 1.9330 (2.0134)\tPrec 24.219% (23.004%)\n",
      "Epoch: [8][200/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 1.9235 (2.0005)\tPrec 23.438% (23.881%)\n",
      "Epoch: [8][300/391]\tTime 0.049 (0.046)\tData 0.001 (0.002)\tLoss 1.9405 (1.9861)\tPrec 25.000% (24.278%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.117 (0.117)\tLoss 1.9581 (1.9581)\tPrec 21.875% (21.875%)\n",
      " * Prec 28.940% \n",
      "best acc: 28.940000\n",
      "Epoch: [9][0/391]\tTime 0.159 (0.159)\tData 0.125 (0.125)\tLoss 1.9445 (1.9445)\tPrec 26.562% (26.562%)\n",
      "Epoch: [9][100/391]\tTime 0.042 (0.047)\tData 0.001 (0.003)\tLoss 1.8912 (1.9120)\tPrec 29.688% (26.864%)\n",
      "Epoch: [9][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 1.9148 (1.9023)\tPrec 27.344% (27.624%)\n",
      "Epoch: [9][300/391]\tTime 0.039 (0.046)\tData 0.002 (0.002)\tLoss 1.8846 (1.8940)\tPrec 30.469% (27.824%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.107 (0.107)\tLoss 1.9421 (1.9421)\tPrec 27.344% (27.344%)\n",
      " * Prec 29.610% \n",
      "best acc: 29.610000\n",
      "Epoch: [10][0/391]\tTime 0.185 (0.185)\tData 0.149 (0.149)\tLoss 1.8936 (1.8936)\tPrec 28.906% (28.906%)\n",
      "Epoch: [10][100/391]\tTime 0.053 (0.047)\tData 0.002 (0.003)\tLoss 1.9088 (1.8147)\tPrec 24.219% (31.366%)\n",
      "Epoch: [10][200/391]\tTime 0.041 (0.046)\tData 0.002 (0.002)\tLoss 1.6925 (1.8160)\tPrec 37.500% (31.429%)\n",
      "Epoch: [10][300/391]\tTime 0.049 (0.046)\tData 0.001 (0.002)\tLoss 1.8278 (1.8045)\tPrec 31.250% (31.935%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.118 (0.118)\tLoss 1.8318 (1.8318)\tPrec 28.125% (28.125%)\n",
      " * Prec 33.080% \n",
      "best acc: 33.080000\n",
      "Epoch: [11][0/391]\tTime 0.173 (0.173)\tData 0.138 (0.138)\tLoss 1.7597 (1.7597)\tPrec 36.719% (36.719%)\n",
      "Epoch: [11][100/391]\tTime 0.045 (0.047)\tData 0.001 (0.003)\tLoss 1.6222 (1.7431)\tPrec 39.062% (35.234%)\n",
      "Epoch: [11][200/391]\tTime 0.043 (0.047)\tData 0.002 (0.002)\tLoss 1.6592 (1.7376)\tPrec 37.500% (35.397%)\n",
      "Epoch: [11][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 1.7557 (1.7233)\tPrec 30.469% (35.771%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.115 (0.115)\tLoss 1.7070 (1.7070)\tPrec 35.938% (35.938%)\n",
      " * Prec 37.370% \n",
      "best acc: 37.370000\n",
      "Epoch: [12][0/391]\tTime 0.168 (0.168)\tData 0.127 (0.127)\tLoss 1.7459 (1.7459)\tPrec 37.500% (37.500%)\n",
      "Epoch: [12][100/391]\tTime 0.046 (0.046)\tData 0.001 (0.003)\tLoss 1.6197 (1.6830)\tPrec 43.750% (37.299%)\n",
      "Epoch: [12][200/391]\tTime 0.043 (0.046)\tData 0.001 (0.002)\tLoss 1.7745 (1.6837)\tPrec 39.844% (37.201%)\n",
      "Epoch: [12][300/391]\tTime 0.051 (0.046)\tData 0.002 (0.002)\tLoss 1.5953 (1.6775)\tPrec 42.188% (37.601%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.114 (0.114)\tLoss 1.6745 (1.6745)\tPrec 39.062% (39.062%)\n",
      " * Prec 39.570% \n",
      "best acc: 39.570000\n",
      "Epoch: [13][0/391]\tTime 0.169 (0.169)\tData 0.133 (0.133)\tLoss 1.7747 (1.7747)\tPrec 34.375% (34.375%)\n",
      "Epoch: [13][100/391]\tTime 0.054 (0.047)\tData 0.001 (0.003)\tLoss 1.5760 (1.6525)\tPrec 35.938% (37.925%)\n",
      "Epoch: [13][200/391]\tTime 0.043 (0.047)\tData 0.002 (0.002)\tLoss 1.6362 (1.6350)\tPrec 39.062% (38.822%)\n",
      "Epoch: [13][300/391]\tTime 0.043 (0.046)\tData 0.002 (0.002)\tLoss 1.5990 (1.6322)\tPrec 35.938% (39.057%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.112 (0.112)\tLoss 1.6769 (1.6769)\tPrec 37.500% (37.500%)\n",
      " * Prec 40.460% \n",
      "best acc: 40.460000\n",
      "Epoch: [14][0/391]\tTime 0.179 (0.179)\tData 0.146 (0.146)\tLoss 1.5533 (1.5533)\tPrec 49.219% (49.219%)\n",
      "Epoch: [14][100/391]\tTime 0.044 (0.047)\tData 0.001 (0.003)\tLoss 1.5098 (1.5959)\tPrec 43.750% (40.625%)\n",
      "Epoch: [14][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 1.6771 (1.5884)\tPrec 38.281% (40.924%)\n",
      "Epoch: [14][300/391]\tTime 0.043 (0.046)\tData 0.002 (0.002)\tLoss 1.6596 (1.5918)\tPrec 42.969% (40.822%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.115 (0.115)\tLoss 1.6475 (1.6475)\tPrec 42.969% (42.969%)\n",
      " * Prec 43.370% \n",
      "best acc: 43.370000\n",
      "Epoch: [15][0/391]\tTime 0.155 (0.155)\tData 0.123 (0.123)\tLoss 1.4985 (1.4985)\tPrec 42.188% (42.188%)\n",
      "Epoch: [15][100/391]\tTime 0.039 (0.046)\tData 0.002 (0.003)\tLoss 1.6321 (1.5487)\tPrec 45.312% (42.752%)\n",
      "Epoch: [15][200/391]\tTime 0.041 (0.046)\tData 0.001 (0.002)\tLoss 1.5744 (1.5365)\tPrec 39.844% (43.163%)\n",
      "Epoch: [15][300/391]\tTime 0.039 (0.045)\tData 0.002 (0.002)\tLoss 1.6862 (1.5347)\tPrec 38.281% (43.426%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.109 (0.109)\tLoss 1.5856 (1.5856)\tPrec 41.406% (41.406%)\n",
      " * Prec 46.790% \n",
      "best acc: 46.790000\n",
      "Epoch: [16][0/391]\tTime 0.150 (0.150)\tData 0.117 (0.117)\tLoss 1.4112 (1.4112)\tPrec 46.875% (46.875%)\n",
      "Epoch: [16][100/391]\tTime 0.053 (0.046)\tData 0.001 (0.003)\tLoss 1.3515 (1.5021)\tPrec 51.562% (44.933%)\n",
      "Epoch: [16][200/391]\tTime 0.036 (0.046)\tData 0.002 (0.002)\tLoss 1.4043 (1.4936)\tPrec 56.250% (45.693%)\n",
      "Epoch: [16][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 1.4266 (1.4797)\tPrec 44.531% (46.159%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.115 (0.115)\tLoss 1.5137 (1.5137)\tPrec 41.406% (41.406%)\n",
      " * Prec 47.380% \n",
      "best acc: 47.380000\n",
      "Epoch: [17][0/391]\tTime 0.179 (0.179)\tData 0.147 (0.147)\tLoss 1.4744 (1.4744)\tPrec 39.062% (39.062%)\n",
      "Epoch: [17][100/391]\tTime 0.041 (0.046)\tData 0.002 (0.003)\tLoss 1.3442 (1.4446)\tPrec 51.562% (46.937%)\n",
      "Epoch: [17][200/391]\tTime 0.042 (0.046)\tData 0.001 (0.002)\tLoss 1.3709 (1.4388)\tPrec 48.438% (47.295%)\n",
      "Epoch: [17][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 1.6764 (1.4322)\tPrec 40.625% (47.685%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.117 (0.117)\tLoss 1.5495 (1.5495)\tPrec 45.312% (45.312%)\n",
      " * Prec 48.640% \n",
      "best acc: 48.640000\n",
      "Epoch: [18][0/391]\tTime 0.162 (0.162)\tData 0.130 (0.130)\tLoss 1.4234 (1.4234)\tPrec 50.781% (50.781%)\n",
      "Epoch: [18][100/391]\tTime 0.047 (0.047)\tData 0.001 (0.003)\tLoss 1.3203 (1.3933)\tPrec 55.469% (49.118%)\n",
      "Epoch: [18][200/391]\tTime 0.047 (0.046)\tData 0.001 (0.002)\tLoss 1.4759 (1.3840)\tPrec 45.312% (49.689%)\n",
      "Epoch: [18][300/391]\tTime 0.038 (0.046)\tData 0.002 (0.002)\tLoss 1.3409 (1.3845)\tPrec 52.344% (49.704%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.118 (0.118)\tLoss 1.5063 (1.5063)\tPrec 43.750% (43.750%)\n",
      " * Prec 51.420% \n",
      "best acc: 51.420000\n",
      "Epoch: [19][0/391]\tTime 0.159 (0.159)\tData 0.126 (0.126)\tLoss 1.1726 (1.1726)\tPrec 50.781% (50.781%)\n",
      "Epoch: [19][100/391]\tTime 0.046 (0.046)\tData 0.001 (0.003)\tLoss 1.2185 (1.3346)\tPrec 53.125% (51.709%)\n",
      "Epoch: [19][200/391]\tTime 0.041 (0.046)\tData 0.001 (0.002)\tLoss 1.2214 (1.3403)\tPrec 57.812% (51.629%)\n",
      "Epoch: [19][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 1.2917 (1.3357)\tPrec 50.781% (51.877%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.118 (0.118)\tLoss 1.3882 (1.3882)\tPrec 53.125% (53.125%)\n",
      " * Prec 50.150% \n",
      "best acc: 51.420000\n",
      "Epoch: [20][0/391]\tTime 0.153 (0.153)\tData 0.121 (0.121)\tLoss 1.4033 (1.4033)\tPrec 49.219% (49.219%)\n",
      "Epoch: [20][100/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 1.2643 (1.2995)\tPrec 51.562% (53.303%)\n",
      "Epoch: [20][200/391]\tTime 0.054 (0.046)\tData 0.001 (0.002)\tLoss 1.4275 (1.2862)\tPrec 46.094% (53.840%)\n",
      "Epoch: [20][300/391]\tTime 0.040 (0.046)\tData 0.002 (0.002)\tLoss 1.1634 (1.2793)\tPrec 56.250% (54.148%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.118 (0.118)\tLoss 1.3945 (1.3945)\tPrec 52.344% (52.344%)\n",
      " * Prec 52.790% \n",
      "best acc: 52.790000\n",
      "Epoch: [21][0/391]\tTime 0.171 (0.171)\tData 0.127 (0.127)\tLoss 1.1970 (1.1970)\tPrec 55.469% (55.469%)\n",
      "Epoch: [21][100/391]\tTime 0.040 (0.046)\tData 0.002 (0.003)\tLoss 1.2246 (1.2433)\tPrec 57.031% (55.685%)\n",
      "Epoch: [21][200/391]\tTime 0.051 (0.046)\tData 0.001 (0.002)\tLoss 1.3125 (1.2344)\tPrec 54.688% (56.028%)\n",
      "Epoch: [21][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 1.3607 (1.2305)\tPrec 51.562% (56.128%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.114 (0.114)\tLoss 1.1771 (1.1771)\tPrec 57.031% (57.031%)\n",
      " * Prec 55.880% \n",
      "best acc: 55.880000\n",
      "Epoch: [22][0/391]\tTime 0.160 (0.160)\tData 0.123 (0.123)\tLoss 1.0847 (1.0847)\tPrec 67.969% (67.969%)\n",
      "Epoch: [22][100/391]\tTime 0.053 (0.046)\tData 0.001 (0.003)\tLoss 1.1882 (1.1879)\tPrec 54.688% (57.403%)\n",
      "Epoch: [22][200/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 1.0947 (1.1875)\tPrec 58.594% (57.556%)\n",
      "Epoch: [22][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 1.2148 (1.1811)\tPrec 52.344% (57.779%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.118 (0.118)\tLoss 1.2780 (1.2780)\tPrec 53.125% (53.125%)\n",
      " * Prec 56.460% \n",
      "best acc: 56.460000\n",
      "Epoch: [23][0/391]\tTime 0.174 (0.174)\tData 0.142 (0.142)\tLoss 1.2289 (1.2289)\tPrec 53.125% (53.125%)\n",
      "Epoch: [23][100/391]\tTime 0.045 (0.047)\tData 0.001 (0.003)\tLoss 1.0330 (1.1340)\tPrec 67.969% (59.329%)\n",
      "Epoch: [23][200/391]\tTime 0.041 (0.046)\tData 0.001 (0.002)\tLoss 1.2102 (1.1361)\tPrec 55.469% (59.247%)\n",
      "Epoch: [23][300/391]\tTime 0.051 (0.046)\tData 0.001 (0.002)\tLoss 1.0902 (1.1348)\tPrec 62.500% (59.282%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.113 (0.113)\tLoss 1.2565 (1.2565)\tPrec 53.125% (53.125%)\n",
      " * Prec 58.880% \n",
      "best acc: 58.880000\n",
      "Epoch: [24][0/391]\tTime 0.168 (0.168)\tData 0.129 (0.129)\tLoss 1.0927 (1.0927)\tPrec 61.719% (61.719%)\n",
      "Epoch: [24][100/391]\tTime 0.042 (0.047)\tData 0.001 (0.003)\tLoss 1.1746 (1.1258)\tPrec 56.250% (60.110%)\n",
      "Epoch: [24][200/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 1.1957 (1.1284)\tPrec 60.938% (60.145%)\n",
      "Epoch: [24][300/391]\tTime 0.054 (0.046)\tData 0.001 (0.002)\tLoss 1.0508 (1.1186)\tPrec 64.062% (60.361%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.107 (0.107)\tLoss 1.1513 (1.1513)\tPrec 59.375% (59.375%)\n",
      " * Prec 59.430% \n",
      "best acc: 59.430000\n",
      "Epoch: [25][0/391]\tTime 0.164 (0.164)\tData 0.132 (0.132)\tLoss 1.0247 (1.0247)\tPrec 63.281% (63.281%)\n",
      "Epoch: [25][100/391]\tTime 0.050 (0.047)\tData 0.001 (0.003)\tLoss 0.9709 (1.0598)\tPrec 67.188% (62.175%)\n",
      "Epoch: [25][200/391]\tTime 0.049 (0.046)\tData 0.001 (0.002)\tLoss 1.0631 (1.0691)\tPrec 58.594% (61.987%)\n",
      "Epoch: [25][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.9455 (1.0719)\tPrec 66.406% (61.999%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.125 (0.125)\tLoss 1.0991 (1.0991)\tPrec 63.281% (63.281%)\n",
      " * Prec 61.030% \n",
      "best acc: 61.030000\n",
      "Epoch: [26][0/391]\tTime 0.173 (0.173)\tData 0.129 (0.129)\tLoss 1.0639 (1.0639)\tPrec 58.594% (58.594%)\n",
      "Epoch: [26][100/391]\tTime 0.041 (0.047)\tData 0.001 (0.003)\tLoss 1.1251 (1.0518)\tPrec 57.812% (62.817%)\n",
      "Epoch: [26][200/391]\tTime 0.043 (0.046)\tData 0.001 (0.002)\tLoss 1.1369 (1.0396)\tPrec 60.938% (63.106%)\n",
      "Epoch: [26][300/391]\tTime 0.049 (0.046)\tData 0.002 (0.002)\tLoss 1.0752 (1.0376)\tPrec 60.938% (63.260%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.111 (0.111)\tLoss 1.0755 (1.0755)\tPrec 60.938% (60.938%)\n",
      " * Prec 62.660% \n",
      "best acc: 62.660000\n",
      "Epoch: [27][0/391]\tTime 0.161 (0.161)\tData 0.126 (0.126)\tLoss 1.0323 (1.0323)\tPrec 61.719% (61.719%)\n",
      "Epoch: [27][100/391]\tTime 0.047 (0.046)\tData 0.001 (0.003)\tLoss 0.9118 (1.0050)\tPrec 67.969% (64.766%)\n",
      "Epoch: [27][200/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 1.0027 (1.0098)\tPrec 63.281% (64.467%)\n",
      "Epoch: [27][300/391]\tTime 0.048 (0.045)\tData 0.001 (0.002)\tLoss 1.0674 (1.0097)\tPrec 61.719% (64.493%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.109 (0.109)\tLoss 1.0037 (1.0037)\tPrec 60.938% (60.938%)\n",
      " * Prec 63.690% \n",
      "best acc: 63.690000\n",
      "Epoch: [28][0/391]\tTime 0.179 (0.179)\tData 0.145 (0.145)\tLoss 0.9579 (0.9579)\tPrec 64.844% (64.844%)\n",
      "Epoch: [28][100/391]\tTime 0.045 (0.047)\tData 0.001 (0.003)\tLoss 0.9625 (0.9685)\tPrec 65.625% (65.470%)\n",
      "Epoch: [28][200/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 0.8930 (0.9716)\tPrec 66.406% (65.835%)\n",
      "Epoch: [28][300/391]\tTime 0.053 (0.046)\tData 0.001 (0.002)\tLoss 0.9590 (0.9738)\tPrec 63.281% (65.887%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.108 (0.108)\tLoss 1.0558 (1.0558)\tPrec 60.156% (60.156%)\n",
      " * Prec 64.500% \n",
      "best acc: 64.500000\n",
      "Epoch: [29][0/391]\tTime 0.162 (0.162)\tData 0.129 (0.129)\tLoss 1.1044 (1.1044)\tPrec 57.812% (57.812%)\n",
      "Epoch: [29][100/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.7879 (0.9525)\tPrec 71.875% (66.832%)\n",
      "Epoch: [29][200/391]\tTime 0.054 (0.046)\tData 0.001 (0.002)\tLoss 0.9242 (0.9585)\tPrec 67.188% (66.799%)\n",
      "Epoch: [29][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.9677 (0.9600)\tPrec 66.406% (66.845%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.117 (0.117)\tLoss 1.1745 (1.1745)\tPrec 57.812% (57.812%)\n",
      " * Prec 63.210% \n",
      "best acc: 64.500000\n",
      "Epoch: [30][0/391]\tTime 0.155 (0.155)\tData 0.118 (0.118)\tLoss 0.9651 (0.9651)\tPrec 68.750% (68.750%)\n",
      "Epoch: [30][100/391]\tTime 0.036 (0.047)\tData 0.002 (0.003)\tLoss 0.9198 (0.9366)\tPrec 67.188% (67.536%)\n",
      "Epoch: [30][200/391]\tTime 0.049 (0.046)\tData 0.001 (0.002)\tLoss 0.9226 (0.9279)\tPrec 66.406% (67.817%)\n",
      "Epoch: [30][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.8692 (0.9324)\tPrec 75.000% (67.694%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.114 (0.114)\tLoss 0.9797 (0.9797)\tPrec 63.281% (63.281%)\n",
      " * Prec 65.220% \n",
      "best acc: 65.220000\n",
      "Epoch: [31][0/391]\tTime 0.182 (0.182)\tData 0.150 (0.150)\tLoss 0.9228 (0.9228)\tPrec 70.312% (70.312%)\n",
      "Epoch: [31][100/391]\tTime 0.044 (0.047)\tData 0.001 (0.003)\tLoss 1.0754 (0.8892)\tPrec 62.500% (69.052%)\n",
      "Epoch: [31][200/391]\tTime 0.049 (0.046)\tData 0.001 (0.002)\tLoss 0.9881 (0.8974)\tPrec 64.062% (69.003%)\n",
      "Epoch: [31][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 0.7959 (0.9015)\tPrec 76.562% (68.997%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.104 (0.104)\tLoss 1.0601 (1.0601)\tPrec 64.844% (64.844%)\n",
      " * Prec 67.090% \n",
      "best acc: 67.090000\n",
      "Epoch: [32][0/391]\tTime 0.176 (0.176)\tData 0.144 (0.144)\tLoss 0.7434 (0.7434)\tPrec 72.656% (72.656%)\n",
      "Epoch: [32][100/391]\tTime 0.044 (0.048)\tData 0.001 (0.003)\tLoss 0.8385 (0.8846)\tPrec 71.094% (69.284%)\n",
      "Epoch: [32][200/391]\tTime 0.049 (0.047)\tData 0.001 (0.002)\tLoss 0.7779 (0.8946)\tPrec 75.000% (68.832%)\n",
      "Epoch: [32][300/391]\tTime 0.043 (0.046)\tData 0.001 (0.002)\tLoss 0.9111 (0.8949)\tPrec 68.750% (68.934%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.122 (0.122)\tLoss 1.0329 (1.0329)\tPrec 65.625% (65.625%)\n",
      " * Prec 65.680% \n",
      "best acc: 67.090000\n",
      "Epoch: [33][0/391]\tTime 0.156 (0.156)\tData 0.123 (0.123)\tLoss 0.7870 (0.7870)\tPrec 75.000% (75.000%)\n",
      "Epoch: [33][100/391]\tTime 0.057 (0.047)\tData 0.001 (0.003)\tLoss 0.8863 (0.8808)\tPrec 68.750% (69.407%)\n",
      "Epoch: [33][200/391]\tTime 0.040 (0.046)\tData 0.002 (0.002)\tLoss 0.7617 (0.8716)\tPrec 74.219% (69.815%)\n",
      "Epoch: [33][300/391]\tTime 0.049 (0.046)\tData 0.001 (0.002)\tLoss 0.8350 (0.8673)\tPrec 69.531% (69.991%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.117 (0.117)\tLoss 1.0493 (1.0493)\tPrec 65.625% (65.625%)\n",
      " * Prec 67.840% \n",
      "best acc: 67.840000\n",
      "Epoch: [34][0/391]\tTime 0.157 (0.157)\tData 0.125 (0.125)\tLoss 0.7672 (0.7672)\tPrec 73.438% (73.438%)\n",
      "Epoch: [34][100/391]\tTime 0.043 (0.048)\tData 0.001 (0.003)\tLoss 0.8841 (0.8496)\tPrec 67.969% (70.846%)\n",
      "Epoch: [34][200/391]\tTime 0.054 (0.047)\tData 0.001 (0.002)\tLoss 0.7294 (0.8442)\tPrec 75.000% (70.888%)\n",
      "Epoch: [34][300/391]\tTime 0.040 (0.046)\tData 0.001 (0.002)\tLoss 0.8882 (0.8491)\tPrec 67.188% (70.606%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.126 (0.126)\tLoss 1.0762 (1.0762)\tPrec 62.500% (62.500%)\n",
      " * Prec 66.210% \n",
      "best acc: 67.840000\n",
      "Epoch: [35][0/391]\tTime 0.165 (0.165)\tData 0.132 (0.132)\tLoss 0.8492 (0.8492)\tPrec 67.969% (67.969%)\n",
      "Epoch: [35][100/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.6810 (0.8135)\tPrec 75.781% (71.627%)\n",
      "Epoch: [35][200/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 0.8706 (0.8201)\tPrec 71.875% (71.440%)\n",
      "Epoch: [35][300/391]\tTime 0.047 (0.046)\tData 0.001 (0.002)\tLoss 0.7275 (0.8224)\tPrec 71.094% (71.338%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.116 (0.116)\tLoss 0.9150 (0.9150)\tPrec 67.969% (67.969%)\n",
      " * Prec 69.080% \n",
      "best acc: 69.080000\n",
      "Epoch: [36][0/391]\tTime 0.166 (0.166)\tData 0.134 (0.134)\tLoss 0.9313 (0.9313)\tPrec 73.438% (73.438%)\n",
      "Epoch: [36][100/391]\tTime 0.047 (0.047)\tData 0.001 (0.003)\tLoss 0.7047 (0.8157)\tPrec 77.344% (71.898%)\n",
      "Epoch: [36][200/391]\tTime 0.050 (0.046)\tData 0.001 (0.002)\tLoss 0.9079 (0.8108)\tPrec 65.625% (72.042%)\n",
      "Epoch: [36][300/391]\tTime 0.036 (0.046)\tData 0.002 (0.002)\tLoss 0.8645 (0.8113)\tPrec 71.875% (72.119%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.109 (0.109)\tLoss 0.8890 (0.8890)\tPrec 67.969% (67.969%)\n",
      " * Prec 72.420% \n",
      "best acc: 72.420000\n",
      "Epoch: [37][0/391]\tTime 0.169 (0.169)\tData 0.133 (0.133)\tLoss 0.8910 (0.8910)\tPrec 66.406% (66.406%)\n",
      "Epoch: [37][100/391]\tTime 0.049 (0.047)\tData 0.001 (0.003)\tLoss 0.8196 (0.7955)\tPrec 71.875% (72.509%)\n",
      "Epoch: [37][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.8586 (0.7896)\tPrec 64.844% (72.847%)\n",
      "Epoch: [37][300/391]\tTime 0.055 (0.046)\tData 0.001 (0.002)\tLoss 0.5367 (0.7841)\tPrec 82.812% (72.963%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.116 (0.116)\tLoss 0.9475 (0.9475)\tPrec 66.406% (66.406%)\n",
      " * Prec 69.790% \n",
      "best acc: 72.420000\n",
      "Epoch: [38][0/391]\tTime 0.164 (0.164)\tData 0.130 (0.130)\tLoss 0.8574 (0.8574)\tPrec 75.000% (75.000%)\n",
      "Epoch: [38][100/391]\tTime 0.042 (0.046)\tData 0.001 (0.003)\tLoss 0.7276 (0.7850)\tPrec 76.562% (73.677%)\n",
      "Epoch: [38][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.6922 (0.7670)\tPrec 76.562% (74.145%)\n",
      "Epoch: [38][300/391]\tTime 0.053 (0.046)\tData 0.001 (0.002)\tLoss 0.9080 (0.7690)\tPrec 75.781% (74.159%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.110 (0.110)\tLoss 0.8141 (0.8141)\tPrec 68.750% (68.750%)\n",
      " * Prec 71.770% \n",
      "best acc: 72.420000\n",
      "Epoch: [39][0/391]\tTime 0.155 (0.155)\tData 0.120 (0.120)\tLoss 0.8629 (0.8629)\tPrec 71.094% (71.094%)\n",
      "Epoch: [39][100/391]\tTime 0.050 (0.046)\tData 0.001 (0.003)\tLoss 0.5092 (0.7582)\tPrec 83.594% (73.708%)\n",
      "Epoch: [39][200/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 0.6446 (0.7493)\tPrec 79.688% (74.328%)\n",
      "Epoch: [39][300/391]\tTime 0.048 (0.046)\tData 0.001 (0.002)\tLoss 0.6697 (0.7444)\tPrec 81.250% (74.590%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.113 (0.113)\tLoss 0.8405 (0.8405)\tPrec 67.969% (67.969%)\n",
      " * Prec 72.260% \n",
      "best acc: 72.420000\n",
      "Epoch: [40][0/391]\tTime 0.150 (0.150)\tData 0.118 (0.118)\tLoss 0.7724 (0.7724)\tPrec 75.781% (75.781%)\n",
      "Epoch: [40][100/391]\tTime 0.040 (0.046)\tData 0.002 (0.003)\tLoss 0.9254 (0.7164)\tPrec 68.750% (75.456%)\n",
      "Epoch: [40][200/391]\tTime 0.042 (0.046)\tData 0.002 (0.002)\tLoss 0.7309 (0.7169)\tPrec 72.656% (75.602%)\n",
      "Epoch: [40][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.6809 (0.7209)\tPrec 81.250% (75.527%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.129 (0.129)\tLoss 0.7992 (0.7992)\tPrec 71.094% (71.094%)\n",
      " * Prec 73.410% \n",
      "best acc: 73.410000\n",
      "Epoch: [41][0/391]\tTime 0.168 (0.168)\tData 0.132 (0.132)\tLoss 0.8544 (0.8544)\tPrec 66.406% (66.406%)\n",
      "Epoch: [41][100/391]\tTime 0.043 (0.046)\tData 0.001 (0.003)\tLoss 0.7839 (0.7060)\tPrec 67.969% (75.743%)\n",
      "Epoch: [41][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.7116 (0.7128)\tPrec 75.000% (75.727%)\n",
      "Epoch: [41][300/391]\tTime 0.043 (0.046)\tData 0.001 (0.002)\tLoss 0.6488 (0.7100)\tPrec 75.781% (75.986%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.114 (0.114)\tLoss 0.7833 (0.7833)\tPrec 73.438% (73.438%)\n",
      " * Prec 72.310% \n",
      "best acc: 73.410000\n",
      "Epoch: [42][0/391]\tTime 0.157 (0.157)\tData 0.124 (0.124)\tLoss 0.5752 (0.5752)\tPrec 81.250% (81.250%)\n",
      "Epoch: [42][100/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.6496 (0.7090)\tPrec 78.906% (75.650%)\n",
      "Epoch: [42][200/391]\tTime 0.040 (0.046)\tData 0.001 (0.002)\tLoss 0.7592 (0.7071)\tPrec 73.438% (75.948%)\n",
      "Epoch: [42][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.6400 (0.7079)\tPrec 77.344% (75.971%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.110 (0.110)\tLoss 0.7544 (0.7544)\tPrec 73.438% (73.438%)\n",
      " * Prec 72.830% \n",
      "best acc: 73.410000\n",
      "Epoch: [43][0/391]\tTime 0.161 (0.161)\tData 0.127 (0.127)\tLoss 0.8246 (0.8246)\tPrec 71.875% (71.875%)\n",
      "Epoch: [43][100/391]\tTime 0.053 (0.046)\tData 0.001 (0.003)\tLoss 0.5578 (0.6684)\tPrec 82.031% (77.498%)\n",
      "Epoch: [43][200/391]\tTime 0.048 (0.046)\tData 0.001 (0.002)\tLoss 0.6976 (0.6774)\tPrec 82.812% (77.142%)\n",
      "Epoch: [43][300/391]\tTime 0.043 (0.046)\tData 0.002 (0.002)\tLoss 0.6954 (0.6764)\tPrec 77.344% (77.154%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.133 (0.133)\tLoss 0.7040 (0.7040)\tPrec 75.781% (75.781%)\n",
      " * Prec 74.430% \n",
      "best acc: 74.430000\n",
      "Epoch: [44][0/391]\tTime 0.170 (0.170)\tData 0.130 (0.130)\tLoss 0.6372 (0.6372)\tPrec 78.125% (78.125%)\n",
      "Epoch: [44][100/391]\tTime 0.045 (0.047)\tData 0.001 (0.003)\tLoss 0.6235 (0.6622)\tPrec 81.250% (77.630%)\n",
      "Epoch: [44][200/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 0.7071 (0.6641)\tPrec 76.562% (77.515%)\n",
      "Epoch: [44][300/391]\tTime 0.050 (0.046)\tData 0.001 (0.002)\tLoss 0.6698 (0.6633)\tPrec 80.469% (77.559%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.114 (0.114)\tLoss 0.7009 (0.7009)\tPrec 76.562% (76.562%)\n",
      " * Prec 75.720% \n",
      "best acc: 75.720000\n",
      "Epoch: [45][0/391]\tTime 0.155 (0.155)\tData 0.124 (0.124)\tLoss 0.6132 (0.6132)\tPrec 75.781% (75.781%)\n",
      "Epoch: [45][100/391]\tTime 0.048 (0.046)\tData 0.001 (0.003)\tLoss 0.7352 (0.6361)\tPrec 73.438% (78.079%)\n",
      "Epoch: [45][200/391]\tTime 0.036 (0.046)\tData 0.002 (0.002)\tLoss 0.6833 (0.6371)\tPrec 78.125% (78.273%)\n",
      "Epoch: [45][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 0.5814 (0.6451)\tPrec 80.469% (78.083%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.112 (0.112)\tLoss 0.6663 (0.6663)\tPrec 76.562% (76.562%)\n",
      " * Prec 77.280% \n",
      "best acc: 77.280000\n",
      "Epoch: [46][0/391]\tTime 0.186 (0.186)\tData 0.141 (0.141)\tLoss 0.5594 (0.5594)\tPrec 85.156% (85.156%)\n",
      "Epoch: [46][100/391]\tTime 0.047 (0.047)\tData 0.001 (0.003)\tLoss 0.6709 (0.6453)\tPrec 77.344% (78.071%)\n",
      "Epoch: [46][200/391]\tTime 0.042 (0.046)\tData 0.002 (0.002)\tLoss 0.5694 (0.6322)\tPrec 83.594% (78.514%)\n",
      "Epoch: [46][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.7050 (0.6396)\tPrec 77.344% (78.320%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.114 (0.114)\tLoss 0.6726 (0.6726)\tPrec 79.688% (79.688%)\n",
      " * Prec 75.440% \n",
      "best acc: 77.280000\n",
      "Epoch: [47][0/391]\tTime 0.155 (0.155)\tData 0.124 (0.124)\tLoss 0.6231 (0.6231)\tPrec 78.125% (78.125%)\n",
      "Epoch: [47][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.003)\tLoss 0.4986 (0.6074)\tPrec 78.906% (79.254%)\n",
      "Epoch: [47][200/391]\tTime 0.043 (0.047)\tData 0.002 (0.002)\tLoss 0.6415 (0.6136)\tPrec 75.781% (79.085%)\n",
      "Epoch: [47][300/391]\tTime 0.045 (0.046)\tData 0.004 (0.002)\tLoss 0.5522 (0.6160)\tPrec 82.031% (79.111%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.117 (0.117)\tLoss 0.7578 (0.7578)\tPrec 75.000% (75.000%)\n",
      " * Prec 73.150% \n",
      "best acc: 77.280000\n",
      "Epoch: [48][0/391]\tTime 0.153 (0.153)\tData 0.121 (0.121)\tLoss 0.6125 (0.6125)\tPrec 80.469% (80.469%)\n",
      "Epoch: [48][100/391]\tTime 0.051 (0.046)\tData 0.001 (0.003)\tLoss 0.6968 (0.6007)\tPrec 78.125% (79.254%)\n",
      "Epoch: [48][200/391]\tTime 0.045 (0.046)\tData 0.006 (0.002)\tLoss 0.6912 (0.6072)\tPrec 77.344% (79.054%)\n",
      "Epoch: [48][300/391]\tTime 0.048 (0.045)\tData 0.001 (0.002)\tLoss 0.4940 (0.6076)\tPrec 83.594% (79.275%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.117 (0.117)\tLoss 0.7314 (0.7314)\tPrec 74.219% (74.219%)\n",
      " * Prec 76.090% \n",
      "best acc: 77.280000\n",
      "Epoch: [49][0/391]\tTime 0.164 (0.164)\tData 0.123 (0.123)\tLoss 0.4928 (0.4928)\tPrec 85.156% (85.156%)\n",
      "Epoch: [49][100/391]\tTime 0.044 (0.047)\tData 0.002 (0.003)\tLoss 0.4838 (0.5828)\tPrec 82.031% (80.469%)\n",
      "Epoch: [49][200/391]\tTime 0.045 (0.046)\tData 0.002 (0.002)\tLoss 0.6138 (0.5921)\tPrec 81.250% (80.166%)\n",
      "Epoch: [49][300/391]\tTime 0.049 (0.046)\tData 0.001 (0.002)\tLoss 0.6890 (0.5925)\tPrec 74.219% (80.103%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.115 (0.115)\tLoss 0.7052 (0.7052)\tPrec 78.125% (78.125%)\n",
      " * Prec 76.080% \n",
      "best acc: 77.280000\n",
      "Epoch: [50][0/391]\tTime 0.154 (0.154)\tData 0.121 (0.121)\tLoss 0.5123 (0.5123)\tPrec 81.250% (81.250%)\n",
      "Epoch: [50][100/391]\tTime 0.042 (0.048)\tData 0.001 (0.003)\tLoss 0.7058 (0.5841)\tPrec 75.781% (80.492%)\n",
      "Epoch: [50][200/391]\tTime 0.042 (0.047)\tData 0.002 (0.002)\tLoss 0.6682 (0.5752)\tPrec 79.688% (80.651%)\n",
      "Epoch: [50][300/391]\tTime 0.052 (0.047)\tData 0.001 (0.002)\tLoss 0.5416 (0.5780)\tPrec 78.906% (80.357%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.121 (0.121)\tLoss 0.5999 (0.5999)\tPrec 80.469% (80.469%)\n",
      " * Prec 77.460% \n",
      "best acc: 77.460000\n",
      "Epoch: [51][0/391]\tTime 0.186 (0.186)\tData 0.152 (0.152)\tLoss 0.6128 (0.6128)\tPrec 79.688% (79.688%)\n",
      "Epoch: [51][100/391]\tTime 0.054 (0.047)\tData 0.001 (0.003)\tLoss 0.4769 (0.5552)\tPrec 82.031% (81.033%)\n",
      "Epoch: [51][200/391]\tTime 0.044 (0.046)\tData 0.002 (0.002)\tLoss 0.6372 (0.5556)\tPrec 78.125% (81.091%)\n",
      "Epoch: [51][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.4997 (0.5635)\tPrec 82.031% (80.780%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.114 (0.114)\tLoss 0.5564 (0.5564)\tPrec 78.906% (78.906%)\n",
      " * Prec 77.250% \n",
      "best acc: 77.460000\n",
      "Epoch: [52][0/391]\tTime 0.184 (0.184)\tData 0.143 (0.143)\tLoss 0.4397 (0.4397)\tPrec 85.156% (85.156%)\n",
      "Epoch: [52][100/391]\tTime 0.049 (0.047)\tData 0.001 (0.003)\tLoss 0.5260 (0.5497)\tPrec 83.594% (80.987%)\n",
      "Epoch: [52][200/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 0.6456 (0.5466)\tPrec 79.688% (81.394%)\n",
      "Epoch: [52][300/391]\tTime 0.048 (0.046)\tData 0.001 (0.002)\tLoss 0.4638 (0.5484)\tPrec 82.031% (81.260%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.127 (0.127)\tLoss 0.7050 (0.7050)\tPrec 75.000% (75.000%)\n",
      " * Prec 76.980% \n",
      "best acc: 77.460000\n",
      "Epoch: [53][0/391]\tTime 0.162 (0.162)\tData 0.122 (0.122)\tLoss 0.5224 (0.5224)\tPrec 79.688% (79.688%)\n",
      "Epoch: [53][100/391]\tTime 0.048 (0.046)\tData 0.001 (0.003)\tLoss 0.4493 (0.5362)\tPrec 85.938% (81.521%)\n",
      "Epoch: [53][200/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 0.7487 (0.5369)\tPrec 75.781% (81.744%)\n",
      "Epoch: [53][300/391]\tTime 0.049 (0.046)\tData 0.001 (0.002)\tLoss 0.4607 (0.5382)\tPrec 82.031% (81.746%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.115 (0.115)\tLoss 0.5798 (0.5798)\tPrec 81.250% (81.250%)\n",
      " * Prec 78.960% \n",
      "best acc: 78.960000\n",
      "Epoch: [54][0/391]\tTime 0.171 (0.171)\tData 0.138 (0.138)\tLoss 0.7137 (0.7137)\tPrec 76.562% (76.562%)\n",
      "Epoch: [54][100/391]\tTime 0.045 (0.047)\tData 0.001 (0.003)\tLoss 0.6106 (0.5295)\tPrec 78.125% (81.915%)\n",
      "Epoch: [54][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.5876 (0.5275)\tPrec 81.250% (82.198%)\n",
      "Epoch: [54][300/391]\tTime 0.056 (0.046)\tData 0.002 (0.002)\tLoss 0.5101 (0.5252)\tPrec 78.125% (82.330%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.116 (0.116)\tLoss 0.5963 (0.5963)\tPrec 78.906% (78.906%)\n",
      " * Prec 78.460% \n",
      "best acc: 78.960000\n",
      "Epoch: [55][0/391]\tTime 0.152 (0.152)\tData 0.118 (0.118)\tLoss 0.6502 (0.6502)\tPrec 75.781% (75.781%)\n",
      "Epoch: [55][100/391]\tTime 0.051 (0.047)\tData 0.001 (0.003)\tLoss 0.4613 (0.5073)\tPrec 82.812% (83.091%)\n",
      "Epoch: [55][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.5581 (0.5194)\tPrec 82.031% (82.622%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, epochs):\n\u001b[1;32m     22\u001b[0m     adjust_learning_rate(optimizer, epoch)\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# evaluate on test set\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation starts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 73\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(trainloader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (\u001b[38;5;28minput\u001b[39m, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# measure data loading time\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     data_time\u001b[38;5;241m.\u001b[39mupdate(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m end)\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28minput\u001b[39m, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, target\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# compute output\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This cell won't be given, but students will complete the training\n",
    "\n",
    "lr = 0.05\n",
    "weight_decay = 1e-4\n",
    "epochs = 250\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name)\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 7896/10000 (79%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/VGG16_quant_2bit1/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceramic-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send an input and grap the value by using prehook like HW3\n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []\n",
    "\n",
    "save_output = SaveOutput()\n",
    "\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        layer.register_forward_pre_hook(save_output)\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spoken-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.layer1[0].conv1)\n",
    "\n",
    "w_bit = 4\n",
    "weight_q = model.features[27].weight_q # quantized value is stored during the training\n",
    "w_alpha = model.features[27].weight_quant.wgt_alpha   # alpha is defined in your model already. bring it out here\n",
    "w_delta = w_alpha/(2**(w_bit-1)-1)    # delta can be calculated by using alpha and w_bit\n",
    "weight_int = weight_q/w_delta # w_int can be calculated by weight_q and w_delta\n",
    "#print(weight_int) # you should see clean integer numbers\n",
    "#weight_int.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interior-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bit = 2\n",
    "x = save_output.outputs[8][0]  # input of the 2nd conv layer\n",
    "x_alpha  = model.features[27].act_alpha\n",
    "x_delta = x_alpha/(2**x_bit-1)\n",
    "\n",
    "act_quant_fn = act_quantization(x_bit) # define the quantization function\n",
    "x_q = act_quant_fn(x, x_alpha)         # create the quantized value for x\n",
    "\n",
    "x_int = x_q/x_delta\n",
    "#print(x_int) # you should see clean integer numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ranging-porter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5333e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conv_int = torch.nn.Conv2d(in_channels = 8, out_channels=16, kernel_size = 3, padding = 1, bias = False)\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "\n",
    "output_int = conv_int(x_int)    # output_int can be calculated with conv_int and x_int\n",
    "output_recovered = output_int*w_delta*x_delta  # recover with x_delta and w_delta\n",
    "#print(output_recovered)\n",
    "l1 = nn.ReLU()\n",
    "output_recovered = l1(output_recovered)\n",
    "\n",
    "output_ref = save_output.outputs[9][0]\n",
    "\n",
    "difference = abs(output_ref - output_recovered)\n",
    "print(difference.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "corresponding-significance",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 36])\n",
      "torch.Size([1, 8, 16, 9])\n",
      "torch.Size([1, 16, 36])\n",
      "torch.Size([1, 8, 16, 9])\n"
     ]
    }
   ],
   "source": [
    "a_int = x_int[0,:,:,:]  # pick only one input out of batch\n",
    "#print(a_int.size())\n",
    "\n",
    "w_int = torch.reshape(weight_int, (weight_int.size(0), weight_int.size(1), -1))\n",
    "#print(w_int.size())\n",
    "\n",
    "padding = 1\n",
    "stride = 1\n",
    "array_size_oc = 8 # row and column number\n",
    "array_size_ic = 16 # row and column number\n",
    "\n",
    "nig = range(a_int.size(1))  ## ni group\n",
    "njg = range(a_int.size(2))  ## nj group\n",
    "\n",
    "icg = range(int(w_int.size(1)))  ## input channel \n",
    "ocg = range(int(w_int.size(0)))  ## output channel\n",
    "\n",
    "ic_tileg = range(int(len(icg)/array_size_ic))\n",
    "oc_tileg = range(int(len(ocg)/array_size_oc))\n",
    "\n",
    "kijg = range(w_int.size(2))\n",
    "ki_dim = int(math.sqrt(w_int.size(2)))  ## Kernel's 1 dim size\n",
    "\n",
    "a_pad = torch.zeros(len(icg), len(nig)+padding*2, len(nig)+padding*2).cuda()\n",
    "#print(a_pad.size()) #[8, , ]\n",
    "\n",
    "a_pad[ :, padding:padding+len(nig), padding:padding+len(njg)] = a_int.cuda()\n",
    "a_pad = torch.reshape(a_pad, (a_pad.size(0), -1))\n",
    "#print(a_pad.size())\n",
    "\n",
    "a_tile = torch.zeros(len(ic_tileg), array_size_ic,    a_pad.size(1)).cuda() \n",
    "w_tile = torch.zeros(len(oc_tileg)*len(ic_tileg), array_size_oc, array_size_ic, len(kijg)).cuda() \n",
    "print(a_tile.size())\n",
    "print(w_tile.size())\n",
    "\n",
    "for ic_tile in ic_tileg:\n",
    "    a_tile[ic_tile,:,:] = a_pad[ic_tile*array_size_ic:(ic_tile+1)*array_size_ic,:]\n",
    "\n",
    "for ic_tile in ic_tileg:\n",
    "    for oc_tile in oc_tileg:\n",
    "        w_tile[oc_tile*len(oc_tileg) + ic_tile,:,:,:] = w_int[oc_tile*array_size_oc:(oc_tile+1)*array_size_oc, ic_tile*array_size_ic:(ic_tile+1)*array_size_ic, :]\n",
    "print(a_tile.size())\n",
    "print(w_tile.size())\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "p_nijg = range(a_pad.size(1)) ## psum nij group\n",
    "\n",
    "psum = torch.zeros(len(ic_tileg), len(oc_tileg), array_size_oc, len(p_nijg), len(kijg)).cuda() \n",
    "\n",
    "for kij in kijg:\n",
    "    for ic_tile in ic_tileg:       # Tiling into array_sizeXarray_size array\n",
    "        for oc_tile in oc_tileg:   # Tiling into array_sizeXarray_size array        \n",
    "            for nij in p_nijg:       # time domain, sequentially given input\n",
    "                    m = nn.Linear(array_size_ic, array_size_oc, bias=False)\n",
    "                    #m.weight = torch.nn.Parameter(w_int[oc_tile*array_size:(oc_tile+1)*array_size, ic_tile*array_size:(ic_tile+1)*array_size, kij])\n",
    "                    m.weight = torch.nn.Parameter(w_tile[len(oc_tileg)*oc_tile+ic_tile,:,:,kij])\n",
    "                    psum[ic_tile, oc_tile, :, nij, kij] = m(a_tile[ic_tile,:,nij]).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "113867a0-5069-4e73-87e5-c64f1c2b8c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "6\n",
      "7\n",
      "8\n",
      "12\n",
      "13\n",
      "14\n",
      "1\n",
      "2\n",
      "3\n",
      "7\n",
      "8\n",
      "9\n",
      "13\n",
      "14\n",
      "15\n",
      "2\n",
      "3\n",
      "4\n",
      "8\n",
      "9\n",
      "10\n",
      "14\n",
      "15\n",
      "16\n",
      "3\n",
      "4\n",
      "5\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "17\n",
      "6\n",
      "7\n",
      "8\n",
      "12\n",
      "13\n",
      "14\n",
      "18\n",
      "19\n",
      "20\n",
      "7\n",
      "8\n",
      "9\n",
      "13\n",
      "14\n",
      "15\n",
      "19\n",
      "20\n",
      "21\n",
      "8\n",
      "9\n",
      "10\n",
      "14\n",
      "15\n",
      "16\n",
      "20\n",
      "21\n",
      "22\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "17\n",
      "21\n",
      "22\n",
      "23\n",
      "12\n",
      "13\n",
      "14\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "25\n",
      "26\n",
      "13\n",
      "14\n",
      "15\n",
      "19\n",
      "20\n",
      "21\n",
      "25\n",
      "26\n",
      "27\n",
      "14\n",
      "15\n",
      "16\n",
      "20\n",
      "21\n",
      "22\n",
      "26\n",
      "27\n",
      "28\n",
      "15\n",
      "16\n",
      "17\n",
      "21\n",
      "22\n",
      "23\n",
      "27\n",
      "28\n",
      "29\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "25\n",
      "26\n",
      "30\n",
      "31\n",
      "32\n",
      "19\n",
      "20\n",
      "21\n",
      "25\n",
      "26\n",
      "27\n",
      "31\n",
      "32\n",
      "33\n",
      "20\n",
      "21\n",
      "22\n",
      "26\n",
      "27\n",
      "28\n",
      "32\n",
      "33\n",
      "34\n",
      "21\n",
      "22\n",
      "23\n",
      "27\n",
      "28\n",
      "29\n",
      "33\n",
      "34\n",
      "35\n",
      "torch.Size([8, 16])\n",
      "torch.Size([1, 1, 8, 36, 9])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "a_pad_ni_dim = int(math.sqrt(a_pad.size(1))) # 32\n",
    "\n",
    "o_ni_dim = int((a_pad_ni_dim - (ki_dim- 1) - 1)/stride + 1)\n",
    "o_nijg = range(o_ni_dim**2)    \n",
    "print(a_pad_ni_dim)\n",
    "print(o_ni_dim)\n",
    "    \n",
    "out = torch.zeros(len(ocg), len(o_nijg)).cuda()\n",
    "  \n",
    "   \n",
    "### SFP accumulation ###\n",
    "for o_nij in o_nijg: \n",
    "    for kij in kijg:\n",
    "        for ic_tile in ic_tileg:    \n",
    "            for oc_tile in oc_tileg:   \n",
    "                out[oc_tile*array_size_oc:(oc_tile+1)*array_size_oc, o_nij] = out[oc_tile*array_size_oc:(oc_tile+1)*array_size_oc, o_nij] + \\\n",
    "                psum[ic_tile, oc_tile, :, int(o_nij/o_ni_dim)*a_pad_ni_dim + o_nij%o_ni_dim + int(kij/ki_dim)*a_pad_ni_dim + kij%ki_dim, kij]\n",
    "\n",
    "print(out.size())\n",
    "print(psum.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fce965ef-9350-4cc8-96bf-18a770e9e1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2398e-05, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_2D = torch.reshape(out, (out.size(0), o_ni_dim, -1))\n",
    "\n",
    "difference = (out_2D - output_int[0,:,:,:])\n",
    "print(difference.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "262da604-4257-439c-94c3-3cdf5c8de552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 36])\n"
     ]
    }
   ],
   "source": [
    "### show this cell partially. The following cells should be printed by students ###\n",
    "tile_id = 0 \n",
    "#nij = 36 # just a random number\n",
    "X = a_tile[tile_id,:,:]  # [tile_num, array row num, time_steps]\n",
    "print(X.size())\n",
    "\n",
    "bit_precision = 2\n",
    "file = open('activation.txt', 'w') #write to file\n",
    "file.write('#time0row15[msb-lsb],time0row14[msb-lst],....,time0row0[msb-lst]#\\n')\n",
    "file.write('#time1row15[msb-lsb],time1row14[msb-lst],....,time1row0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(X.size(1)):  # time step\n",
    "    for j in range(X.size(0)): # row #\n",
    "        X_bin = '{0:02b}'.format(round(X[15-j,i].item()))\n",
    "        for k in range(bit_precision):\n",
    "            file.write(X_bin[k])        \n",
    "        #file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file    \n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06daa275-7a9a-478d-bddd-61a13f468018",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 16, 9])\n"
     ]
    }
   ],
   "source": [
    "tile_id = 0 \n",
    "kij = 0\n",
    "W = w_tile[tile_id,:,:,kij]  # w_tile[tile_num, array col num, array row num, kij]\n",
    "\n",
    "bit_precision = 4\n",
    "print(w_tile.size())\n",
    "\n",
    "for kij in range(w_tile.size(3)):\n",
    "    W = w_tile[tile_id,:,:,kij]\n",
    "    file = open('weight_kij'+str(kij)+'_reference.txt', 'w') #write to file\n",
    "    file.write('#col0row15[msb-lsb],col0row14[msb-lst],....,col0row0[msb-lst]#\\n')\n",
    "    file.write('#col1row15[msb-lsb],col1row14[msb-lst],....,col1row0[msb-lst]#\\n')\n",
    "    file.write('#................#\\n')\n",
    "    for i in range(W.size(0)):\n",
    "        for j in range(W.size(1)):\n",
    "            temp = round(W[i,15-j].item())\n",
    "            if temp<0:\n",
    "                temp=temp+16\n",
    "            W_bin = '{0:04b}'.format(temp)\n",
    "            for k in range(bit_precision):\n",
    "                file.write(W_bin[k])        \n",
    "            #file.write(' ')  # for visibility with blank between words, you can use\n",
    "        file.write('\\n')\n",
    "    file.close() #close file \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f96c72ef-e502-4e50-9e02-ab358b4050f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 16, 9])\n"
     ]
    }
   ],
   "source": [
    "tile_id = 0 \n",
    "kij = 0\n",
    "W = w_tile[tile_id,:,:,kij]  # w_tile[tile_num, array col num, array row num, kij]\n",
    "\n",
    "bit_precision = 4\n",
    "print(w_tile.size())\n",
    "\n",
    "for kij in range(w_tile.size(3)):\n",
    "    W = w_tile[tile_id, :, :, kij]\n",
    "    file = open('weight_kij'+str(kij)+'.txt', 'w') #write to file\n",
    "    file.write('#col0row14[msb-lsb],col0row12[msb-lst],....,col0row0[msb-lst]#\\n')\n",
    "    file.write('#col0row15[msb-lsb],col0row13[msb-lst],....,col0row1[msb-lst]#\\n')\n",
    "    file.write('#................#\\n')\n",
    "    for i in range(W.size(0)):\n",
    "        #Even\n",
    "        for j in range(W.size(1)):\n",
    "            if j % 2 != 0:\n",
    "                temp = round(W[i, 15-j].item())\n",
    "                if temp < 0:\n",
    "                    temp = temp + 2**bit_precision\n",
    "                W_bin = '{:04b}'.format(temp)\n",
    "                for k in range(bit_precision):\n",
    "                    file.write(W_bin[k])  \n",
    "        file.write('\\n')\n",
    "        #Odd\n",
    "        for j in range(W.size(1)):\n",
    "            if j % 2 == 0:\n",
    "                temp = round(W[i, 15-j].item())\n",
    "                if temp < 0:\n",
    "                    temp = temp + 2**bit_precision\n",
    "                W_bin = '{:04b}'.format(temp)\n",
    "                for k in range(bit_precision):\n",
    "                    file.write(W_bin[k])  \n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "767603f6-5ca7-4aff-94af-7469498be40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.0000, -7.0000,  7.0000,  3.0000,  0.0000,  0.0000,  0.0000, -1.0000,\n",
       "          4.0000,  2.0000,  2.0000,  0.0000,  1.0000, -2.0000,  4.0000,  0.0000],\n",
       "        [ 0.0000, -1.0000, -4.0000, -0.0000,  6.0000, -2.0000, -0.0000, -0.0000,\n",
       "         -1.0000,  2.0000,  7.0000, -0.0000, -1.0000,  3.0000, -5.0000, -4.0000],\n",
       "        [-2.0000, -7.0000, -6.0000,  1.0000,  4.0000,  2.0000,  1.0000,  6.0000,\n",
       "          1.0000, -0.0000, -7.0000,  0.0000, -0.0000, -3.0000, -4.0000,  7.0000],\n",
       "        [ 2.0000,  1.0000, -1.0000, -0.0000,  4.0000, -1.0000,  0.0000, -1.0000,\n",
       "         -0.0000, -3.0000,  2.0000,  0.0000,  0.0000,  1.0000,  0.0000,  3.0000],\n",
       "        [-3.0000,  7.0000,  7.0000, -1.0000, -2.0000,  7.0000,  1.0000,  7.0000,\n",
       "          3.0000,  7.0000,  1.0000, -0.0000, -0.0000, -7.0000,  7.0000, -3.0000],\n",
       "        [ 5.0000, -4.0000,  7.0000,  0.0000,  4.0000,  2.0000,  3.0000, -3.0000,\n",
       "          7.0000,  5.0000,  3.0000,  0.0000,  1.0000, -3.0000, -2.0000,  6.0000],\n",
       "        [ 3.0000, -3.0000, -7.0000,  0.0000,  2.0000, -1.0000, -0.0000,  6.0000,\n",
       "          7.0000,  5.0000,  7.0000,  1.0000,  0.0000,  3.0000, -1.0000, -3.0000],\n",
       "        [ 2.0000, -2.0000, -6.0000,  1.0000,  7.0000,  3.0000, -1.0000, -1.0000,\n",
       "          7.0000, -2.0000,  1.0000,  1.0000,  1.0000,  3.0000,  1.0000, -3.0000]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[:,:] # check this number with your 2nd line in weight.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df09f531-a569-417b-a936-a3416ecf1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete this cell ###\n",
    "ic_tile_id = 0 \n",
    "oc_tile_id = 0 \n",
    "#print(psum.size())\n",
    "\n",
    "for kij in range(psum.size(4)):\n",
    "    psum_tile = psum[ic_tile_id,oc_tile_id,:,:,kij]\n",
    "    bit_precision = 16\n",
    "    file = open('psum'+str(kij)+'.txt', 'w') #write to file\n",
    "    file.write('#time0col17[msb-lsb],time0col16[msb-lst],....,time0col0[msb-lst]#\\n')\n",
    "    file.write('#time1col17[msb-lsb],time1col16[msb-lst],....,time1col0[msb-lst]#\\n')\n",
    "    file.write('#................#\\n')\n",
    "\n",
    "    for i in range(psum_tile.size(1)):  # time step\n",
    "        for j in range(psum_tile.size(0)): # row #\n",
    "            if round(psum_tile[7-j,i].item())<0:\n",
    "                psum_bin = '{0:04X}'.format(round(psum_tile[7-j,i].item()+2**bit_precision))\n",
    "            else:\n",
    "                psum_bin = '{0:04X}'.format(round(psum_tile[7-j,i].item()))\n",
    "            for k in range(4):\n",
    "                file.write(psum_bin[k])        \n",
    "            file.write(' ')  # for visibility with blank between words, you can use\n",
    "        file.write('\\n')\n",
    "    file.close() #close file  \n",
    "\n",
    "#psum_tile[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "169091f4-c4fd-4420-82a7-6a5807b3564d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16])\n"
     ]
    }
   ],
   "source": [
    "bit_precision = 16\n",
    "file = open('out.txt', 'w') #write to file\n",
    "file.write('#time0col7[msb-lsb],time0col6[msb-lst],....,time0col0[msb-lst]#\\n')\n",
    "file.write('#time1col7[msb-lsb],time1col6[msb-lst],....,time1col0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "# relu\n",
    "out = torch.relu(torch.reshape(output_int[0,:,:,:], (output_int[0,:,:,:].size(0), -1)))\n",
    "print(out.size())\n",
    "for i in range(out.size(1)):  # nijg #\n",
    "    for j in range(out.size(0)): # row #\n",
    "        temp=round(out[7-j,i].item())\n",
    "        if(temp < 0 ):\n",
    "            temp=temp+2**bit_precision\n",
    "        W_bin = '{0:016b}'.format(temp)\n",
    "        for b in range(bit_precision):\n",
    "            file.write(W_bin[b])        \n",
    "        #file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c70c5-61ca-483d-b4a0-d6cec6a23085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
